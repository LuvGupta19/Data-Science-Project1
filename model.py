# -*- coding: utf-8 -*-
"""Techno-Lending Club.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yy7aSeK1xSOPXQKyRAs3cDT1Gm6cTNhN

# **Technocolab project**
## Luv Gupta
"""

#Mounting Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""# Gathering, Accessing and Cleaning of Data """

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline   
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score

df=pd.read_csv("/content/drive/MyDrive/techno/Lending_Club_Loan_approval_Optimization.csv")

df.head(2)

#Drop 1st column
df=df.iloc[:,1:]

df.head(2)

df.shape

df.info()

df.describe()

x=df.columns
df.rename(columns={"Amount Requested":"Amount_Requested", "Debt-To-Income Ratio":"Debt_To_Income_Ratio", "Employment Length": "Employment_Length"}, inplace=True)

df.head(2)

df.isnull().sum()
#No null in any column

df.duplicated().sum()

#Drop duplicated Data
df.drop_duplicates(inplace=True)
df.duplicated().sum()

df.corr()

"""# Exploratory Data Analysis:

Co-relation Matrix
"""

sns.heatmap(df.corr(), cmap="Reds");
plt.xticks(rotation=60);

#sns.pairplot(data=df)
#Takes huge time to run

"""## Univariate Exploration

### Amount Requested
"""

df.query("Amount_Requested < 45000").size / df.size

"""99 % of data have Amount_Requested less than 45000.
We are going to observe this 99% of data through histogram.
"""

bins=np.arange(df['Amount_Requested'].min(), df['Amount_Requested'].max()+2000, 2000)
plt.hist(data=df, x="Amount_Requested", bins=bins);
plt.xlim(0, 44000)
plt.xticks(np.arange(0, 43000, 6000));
plt.ylabel("Count", size=12)
plt.xlabel("Amount Requested", size=12)
plt.title("Histogram of Amount Requested", size=14);

"""### Risk Score"""

bins=np.arange(df['Risk_Score'].min(), df['Risk_Score'].max()+20, 20)
plt.hist(data=df, x="Risk_Score", bins=bins, color=sns.color_palette()[2]);
plt.xlim(440,880)
plt.xticks(np.arange(440, 880, 40));
plt.title("Risk Score Distribution", size=13)
plt.ylabel("Count", size=12)
plt.xlabel("Risk Score", size=12);

"""### Employment Length

"""

sns.countplot(data=df, x="Employment_Length", color=sns.color_palette()[1]);
plt.ylabel("Count", size=12)
plt.xlabel("Employment_Length", size=12)
plt.title("Employment Length Distribution", size=14);

"""### Debt To Income Ratio"""

df.query("Debt_To_Income_Ratio < 140").size / df.size

"""98 % of data have Debt_To_Income_Ratio less than 140.
We are going to observe this 98% of data through histogram.
"""

bins=np.arange(df['Debt_To_Income_Ratio'].min(), df['Debt_To_Income_Ratio'].max()+5, 5)
plt.hist(data=df, x="Debt_To_Income_Ratio", color=sns.color_palette()[3], bins=bins);
plt.xlim(0,140)
plt.ylabel("Count", size=12)
plt.xlabel("Debt_To_Income_Ratio", size=12)
plt.title("Debt To Income Ratio Distribution", size=14);

"""### Target"""

sns.countplot(data=df, x="Target", color=sns.color_palette()[7]);
plt.ylabel("Count", size=12)
plt.xlabel("Target", size=12)
plt.title("Target Distribution", size=14);

"""## Bivariate Exploration
**Here plots of Target vs other features will be plotted.**

### Target vs Employment Length
"""

plt.figure(figsize=[8,5])
sns.countplot(data=df, x="Employment_Length", palette="Accent", hue="Target");
plt.ylabel("Count", size=12)
plt.xlabel("Employment_Length", size=12)
plt.title("Target vs Employment Length", size=14);

"""### Target vs Risk Score"""

bins=np.arange(440, 880+20, 20)
g=sns.FacetGrid(data=df, col="Target", size=4, aspect=1.5)
plt.xlim(440,880)
g.map(plt.hist, "Risk_Score", bins=bins, color=sns.color_palette()[5]);

"""### Target vs Amount Requested"""

plt.figure(figsize=[8,4])
sns.barplot(data=df, x="Target", y="Amount_Requested", color=sns.color_palette()[4]);
plt.ylabel("mean (Amount Requested)", size=12);   
plt.xlabel("Target", size=12)
plt.title("Target vs mean(Amount Requested)", size=14);

"""### Target vs Debt To Income Ratio"""

bins=np.arange(0, 140+5, 5)
plt.figure(figsize=[8,5])
g=sns.FacetGrid(data=df, col="Target", size=4, aspect=1.3);
plt.xlim(0,140);
g.map(plt.hist, "Debt_To_Income_Ratio", bins=bins);

"""# Splitting our target from rest of features"""

X=df.iloc[:,:-1].values
y=df.iloc[:,-1].values

"""# Getting test and train datasets"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

"""# Applying Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""# Building Models

## **Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix : ")
print(cm)
print()
print(" Accuracy Score: {}%".format(round(accuracy_score(y_test,y_pred)*100,2)))

"""K- Fold Cross Validation"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""## **K- Nearest Neighbors (KNN)**"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print("Confusion MAtrix : ")
print(cm)
print()
print(" Accuracy Score: {}%".format(round(accuracy_score(y_test,y_pred)*100,2)))

"""K- Fold Cross Validation"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""Applying Grid Search to find the best model and the best parameters"""

from sklearn.model_selection import GridSearchCV
parameters = [{'n_neighbors': [4,5,6,7,8,9,10], 'weights':['uniform', 'distance']}]
grid_search = GridSearchCV(estimator = classifier,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 10,
                           n_jobs = -1)
grid_search.fit(X_train, y_train)
best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
print("Best Accuracy: {:.2f} %".format( best_accuracy*100))
print("Best Parameters:", best_parameters)

"""## **SVM && Kernel SVM**"""

#Taking so much time in Execution
#from sklearn.svm import SVC
#classifier = SVC(kernel = 'rbf', random_state = 0)
#classifier.fit(X_train, y_train)

"""## **Naive Bayes**"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix : ")
print(cm)
print()
print(" Accuracy Score: {}%".format(round(accuracy_score(y_test,y_pred)*100,2)))

"""## **Decision Tree Classification**"""

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix : ")
print(cm)
print()
print(" Accuracy Score: {}%".format(round(accuracy_score(y_test,y_pred)*100,2)))

"""## **Random Forest Classification**"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix : ")
print(cm)
print()
print(" Accuracy Score: {:.2f}%".format(accuracy_score(y_test,y_pred)*100))

"""K- Fold Cross Validation"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs=-1)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""## **XGBoost** """

from xgboost import XGBClassifier
xgb = XGBClassifier()
xgb.fit(X_train,y_train)

y_pred=xgb.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix : ")
print(cm)
print()
print(" Accuracy Score: {}%".format(round(accuracy_score(y_test,y_pred)*100,2)))

y_pred=xgb.predict(X_train)
confusion_matrix(y_train, y_pred)

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs=-1)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""K- Fold Cross Validation

## **Catboost**
"""

from catboost import CatBoostClassifier
classifier = CatBoostClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix : ")
print(cm)
print()
print(" Accuracy Score: {:.2f}%".format(accuracy_score(y_test,y_pred)*100))

"""# Making Pickle File of the model with best Accuracy"""

import pickle

pickle.dump(classifier, open('model.pkl','wb'))